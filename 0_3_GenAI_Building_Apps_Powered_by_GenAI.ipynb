{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. prompt Engineering/Designing then create API --> generic LLM\n",
    "2. <b>RAG - Retrieval Augmentation Generation(Personal Data)</b>\n",
    "    - Ecosystem LangChain\n",
    "3. Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independent (X) ----------> Model ------------> Dependent(y)\n",
    "- The relation between Independent and dependent is called model(Algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- machine translation\n",
    "- Summarization\n",
    "- Q and A system\n",
    "- POS tagging\n",
    "- NER\n",
    "\n",
    "above all these are done by the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do you build a Large Language Model?\n",
    "- Huge amount of language/text data\n",
    "- Gen AI Algorithms like <b>Transformers( BERT, GPT)</b> (encoder and decoder(RNN/ LSTM, GRU))\n",
    "    - encoding/embeding - 768/1024 tokens\n",
    "    - Gen AI/ Open AI - GPT-4 - 8191 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM's : open AI - GPT,\n",
    "Meta    - Llama,\n",
    "google  - gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention_all_you_need : https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to build a machine translation/ text summarization/ QA System etc using Gen AI..?\n",
    "1. Identify a good large language model(openAI-GPT, Google-Gemini, Meta-Llama)\n",
    "2. Experiment with the following\n",
    "    - Prompt Engineering\n",
    "    - RAG\n",
    "    - Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>text input  -->  Prompt Engineering ---> LLM ----> O/p </b>\n",
    "- we will get the input as a text data\n",
    "- to build a LLM Model we will use Prompt Engineering(Design)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Prompt?\n",
    "- input to an LLM\n",
    "- prompt engineering / Desigining is about providing input to LLM which can help us to get the desired result and Improve the performance of Language Models\n",
    "\n",
    "#### Type of Prompt Engineering\n",
    "\n",
    "certain best practices to follow\n",
    "1. write a clear instruction\n",
    "    - ex: correct the grammer of the following sentence:  input : They went to market and buy mango\n",
    "    - unclear input : tell me about the logistic regression   ,clear input : give me an introduction about Logistic Regression, Assume that i have no knowledge about Machine Learning. Describe the architecture in simple words using multiple examples if possible and use simple language.\n",
    "2. Few/zero shot learning\n",
    "3. Output Structuring/ Formatting\n",
    "4. Promting personas(Personality)\n",
    "5. LLM's are zero shot Reasoners(i.e give the model time to think)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Example for few shot learning</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review : This is one of the worst movie \n",
    "sentiment: Negative\n",
    "    \n",
    "Review : this movie is simple awesome\n",
    "sentiment: positive\n",
    "    \n",
    "Review : this movie is a documentary on oceans\n",
    "sentiment: neutral\n",
    "    \n",
    "- Review : one of the pathetic movie\n",
    "- Review  : this movie had a lot of flaws\n",
    "- Review  : this movie was not at all good\n",
    "- Review : this documentary is about India's culture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Example for zero shot learning</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgetpass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m getpass\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mgetpass\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEnter your OpenAI API Key : \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ipykernel\\kernelbase.py:1265\u001b[39m, in \u001b[36mKernel.getpass\u001b[39m\u001b[34m(self, prompt, stream)\u001b[39m\n\u001b[32m   1258\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m   1260\u001b[39m     warnings.warn(\n\u001b[32m   1261\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `stream` parameter of `getpass.getpass` will have no effect when using ipykernel\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1262\u001b[39m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m   1263\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m   1264\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1265\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1267\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1268\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1270\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1323\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1324\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1325\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1327\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "openai_api_key = getpass(\"Enter your OpenAI API Key : \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'openai_api_key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HTML\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m openai.api_key = \u001b[43mopenai_api_key\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'openai_api_key' is not defined"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from IPython.display import HTML\n",
    "\n",
    "openai.api_key = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user defined function to use prompt engineering method\n",
    "def get_completion(prompt, model='gpt-4o-mini'):\n",
    "    messages = [{\"role\":\"user\", \"content\": prompt}]\n",
    "    response = openai.chat.completions.create(model=model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    f\"\"\"\n",
    "    Just received the Bluetooth speaker I ordered for beach outings, and it's fantastic.\n",
    "    The sound quality is impressively clear with just the right amount of bass.\n",
    "    It's also waterproof, which tested true during a recent splashing incident.\n",
    "    Though it's compact, the volume can really fill the space.\n",
    "    The price was a bargain for such high-quality sound.\n",
    "    Shipping was also on point, arriving two days early in secure packaging.\n",
    "    \"\"\",\n",
    "    f\"\"\"\n",
    "    Needed a new kitchen blender, but this model has been a nightmare.\n",
    "    It's supposed to handle various foods, but it struggles with anything tougher than cooked vegetables.\n",
    "    It's also incredibly noisy, and the 'easy-clean' feature is a joke; food gets stuck under the blades constantly.\n",
    "    I thought the brand meant quality, but this product has proven me wrong.\n",
    "    Plus, it arrived three days late. Definitely not worth the expense.\n",
    "    \"\"\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m reviews:\n\u001b[32m      3\u001b[39m     prompt =\u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33m    Act as a product review analyst.\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33m    Given the following review, Display the overall sentiment for the review as only one of the following:\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33m    Positive, Negative or Neutral\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33m    \u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreview\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     response = \u001b[43mget_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgpt-4o-mini\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     responses.append(response)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mget_completion\u001b[39m\u001b[34m(prompt, model)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_completion\u001b[39m(prompt, model=\u001b[33m'\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m      3\u001b[39m     messages = [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt}]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     response = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m.create(model=model, messages=messages)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\_utils\\_proxy.py:20\u001b[39m, in \u001b[36mLazyProxy.__getattr__\u001b[39m\u001b[34m(self, attr)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     proxied = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_proxied__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(proxied, LazyProxy):\n\u001b[32m     22\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m proxied  \u001b[38;5;66;03m# pyright: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\_utils\\_proxy.py:58\u001b[39m, in \u001b[36mLazyProxy.__get_proxied__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__get_proxied__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> T:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__load__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\_module_client.py:34\u001b[39m, in \u001b[36mChatProxy.__load__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__load__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Chat:\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.chat\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\__init__.py:349\u001b[39m, in \u001b[36m_load_client\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    333\u001b[39m         _client = _AzureModuleClient(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    334\u001b[39m             api_version=api_version,\n\u001b[32m    335\u001b[39m             azure_endpoint=azure_endpoint,\n\u001b[32m   (...)\u001b[39m\u001b[32m    345\u001b[39m             http_client=http_client,\n\u001b[32m    346\u001b[39m         )\n\u001b[32m    347\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _client\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m     _client = \u001b[43m_ModuleClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m        \u001b[49m\u001b[43morganization\u001b[49m\u001b[43m=\u001b[49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwebhook_secret\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwebhook_secret\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdefault_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdefault_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    361\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _client\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _client\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\_client.py:130\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    128\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    131\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m     )\n\u001b[32m    133\u001b[39m \u001b[38;5;28mself\u001b[39m.api_key = api_key\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "for review in reviews:\n",
    "    prompt =f\"\"\"\n",
    "    Act as a product review analyst.\n",
    "    Given the following review, Display the overall sentiment for the review as only one of the following:\n",
    "    Positive, Negative or Neutral\n",
    "    '''{review}'''\n",
    "    \"\"\"\n",
    "    response = get_completion(prompt, model='gpt-4o-mini')\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review, response in zip(reviews, responses):\n",
    "    print(\"Review :\", review)\n",
    "    print(\"Sentiment :\", response)\n",
    "    print(\"********************\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m reviews:\n\u001b[32m      3\u001b[39m     prompt =\u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33m    Act as a product review analyst.\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33m    Given the following review,\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m \u001b[33m    \u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreview\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     response = \u001b[43mget_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgpt-4o-mini\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     responses.append(response)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mget_completion\u001b[39m\u001b[34m(prompt, model)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_completion\u001b[39m(prompt, model=\u001b[33m'\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m      3\u001b[39m     messages = [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt}]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     response = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m.create(model=model, messages=messages)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\_utils\\_proxy.py:20\u001b[39m, in \u001b[36mLazyProxy.__getattr__\u001b[39m\u001b[34m(self, attr)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     proxied = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_proxied__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(proxied, LazyProxy):\n\u001b[32m     22\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m proxied  \u001b[38;5;66;03m# pyright: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\_utils\\_proxy.py:58\u001b[39m, in \u001b[36mLazyProxy.__get_proxied__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__get_proxied__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> T:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__load__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\_module_client.py:34\u001b[39m, in \u001b[36mChatProxy.__load__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__load__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Chat:\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.chat\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\__init__.py:349\u001b[39m, in \u001b[36m_load_client\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    333\u001b[39m         _client = _AzureModuleClient(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    334\u001b[39m             api_version=api_version,\n\u001b[32m    335\u001b[39m             azure_endpoint=azure_endpoint,\n\u001b[32m   (...)\u001b[39m\u001b[32m    345\u001b[39m             http_client=http_client,\n\u001b[32m    346\u001b[39m         )\n\u001b[32m    347\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _client\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m     _client = \u001b[43m_ModuleClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m        \u001b[49m\u001b[43morganization\u001b[49m\u001b[43m=\u001b[49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwebhook_secret\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwebhook_secret\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdefault_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdefault_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    361\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _client\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _client\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\_client.py:130\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    128\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    131\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m     )\n\u001b[32m    133\u001b[39m \u001b[38;5;28mself\u001b[39m.api_key = api_key\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "for review in reviews:\n",
    "    prompt =f\"\"\"\n",
    "    Act as a product review analyst.\n",
    "    Given the following review,\n",
    "    Display only the overall sentiment for the review:\n",
    "\n",
    "    Try to classify it by using the following examples as a reference:\n",
    "\n",
    "    Review: Just received OnePlus 13 Mobile I ordered for work, and it's amazing.\n",
    "    Sentiment: Positive\n",
    "\n",
    "    Review: Needed a new Mac Laptop, but this model has been totally disappointing.\n",
    "    Sentiment: Negative.\n",
    "    '''{review}'''\n",
    "    \"\"\"\n",
    "    response = get_completion(prompt, model='gpt-4o-mini')\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review, response in zip(reviews, responses):\n",
    "    print(\"Review :\", review)\n",
    "    print(\"Sentiment :\", response)\n",
    "    print(\"********************\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RLHF Model\n",
    "- in GenAI whether the model is good or bad is decided by human feedback\n",
    "- Evaluation Matrix, Deep Eval, Ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Output Structuring\n",
    "- please correct the grammer of this sentence. if there are multiple correction options, give them in numbered list:\n",
    "- sentence : I am waiting for your response.\n",
    "\n",
    "- please correct the grammer of this sentence. if there are multiple correction options, give them in a valid json format:\n",
    "- sentence : I am waiting for your response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Promting personas(Personality)\n",
    "- Answer the question as if you were a <b>rude</b> education counsellor\n",
    "- Question : do you provide the datascience course?\n",
    "\n",
    "- Answer the question as if you were a <b>polite</b> education counsellor\n",
    "- Question : do you provide the datascience course?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. LLM's are zero shot Reasoners(i.e give the model time to think)\n",
    "- 5586*989\n",
    "- what is 5586 multiplied by 989? Let's think step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations of LLM\n",
    "- limited Knowledge : we will not get the answers to recent questions\n",
    "- Halucination (illusion) : two faces and vase image\n",
    "  \n",
    "![illusion](data:image/jpeg;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>To Overcome above limitations we use RAG</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st LLM Model : OpenAI\n",
    "- create / login to OpenAI Account : https://platform.openai.com/docs/overview\n",
    "- make sure you have credits to access the model go to --> Organization --> billing\n",
    "- create API key to access GPT, 4o, 4o-mini, 4, wishper, TexttoSpeech, Embeding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgetpass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m getpass\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m openai_api_key = \u001b[43mgetpass\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEnter your OpenAI API key here :\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ipykernel\\kernelbase.py:1265\u001b[39m, in \u001b[36mKernel.getpass\u001b[39m\u001b[34m(self, prompt, stream)\u001b[39m\n\u001b[32m   1258\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m   1260\u001b[39m     warnings.warn(\n\u001b[32m   1261\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `stream` parameter of `getpass.getpass` will have no effect when using ipykernel\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1262\u001b[39m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m   1263\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m   1264\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1265\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1267\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1268\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1270\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1323\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1324\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1325\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1327\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "openai_api_key = getpass(\"Enter your OpenAI API key here :\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method2 : if you're using VS code\n",
    "- create a .env file  openai_api_key=\"\"\n",
    "- install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'openai_api_key'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dotenv_values\n\u001b[32m      4\u001b[39m config = dotenv_values(\u001b[33m\"\u001b[39m\u001b[33m.env\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m client = OpenAI(api_key=\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mopenai_api_key\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[31mKeyError\u001b[39m: 'openai_api_key'"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "client = OpenAI(api_key=config['openai_api_key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MEthod3: if it is stored in a some file in Nas path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "os.getcwd()\n",
    "os.chdir(\"E:\\\\code\\\\GenerativeAI\\\\data\")\n",
    "f = open('key.txt')\n",
    "OPENAI_API_KEY = f.read()\n",
    "\n",
    "match = re.search(r'openai_api_key:\"([^\"]+)\"', OPENAI_API_KEY)\n",
    "if match:\n",
    "    openai_api_key = match.group(1)\n",
    "    #print(openai_api_key)\n",
    "else:\n",
    "    print(\"API key not found\")\n",
    "\n",
    "#client = OpenAI(api_key = openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key = openai_api_key)\n",
    "client.models.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model='gpt-4o-mini',\n",
    "                                         messages=[{'role':'user',\n",
    "                                                   'content':'Generate 5 Machine Learning Interview Questions and Answers for MCQ Test.'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mresponse\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresponse\u001b[49m.choices[\u001b[32m0\u001b[39m].message.content)\n",
      "\u001b[31mNameError\u001b[39m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m prompt = \u001b[33m\"\"\"\u001b[39m\u001b[33mGive me a numbered list of all the machine learning, deep learning, reinforcement learning and \u001b[39m\n\u001b[32m      2\u001b[39m \u001b[33m    data science modules one should learn to become data scientist?\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgpt-4o-mini\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                                                    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.choices[\u001b[32m0\u001b[39m].message.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1087\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1044\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1045\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1046\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1084\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1085\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1086\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1087\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1089\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1096\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1097\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1098\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1099\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1100\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1113\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1115\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1117\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1118\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1121\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1122\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1124\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\_base_client.py:1249\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1235\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1236\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1237\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1244\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1245\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1246\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1247\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1248\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\_base_client.py:1037\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1034\u001b[39m             err.response.read()\n\u001b[32m   1036\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1041\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Give me a numbered list of all the machine learning, deep learning, reinforcement learning and \n",
    "    data science modules one should learn to become data scientist?\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(model='gpt-4o-mini',\n",
    "                                         messages=[{'role':'user',\n",
    "                                                    \"content\":prompt}],\n",
    "                                         max_tokens=100, stop='5', temperature=)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# temparature : static or creativity and value ranges from 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"what is the newtons 3rd law\"\"\"\n",
    "response = client.chat.completions.create(model='gpt-4o-mini',\n",
    "                                         messages=[{'role':'user',\n",
    "                                                    \"content\":prompt}],\n",
    "                                         temperature=0.1)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"what is the newtons 3rd law\"\"\"\n",
    "response = client.chat.completions.create(model='gpt-4o-mini',\n",
    "                                         messages=[{'role':'user',\n",
    "                                                    \"content\":prompt}],\n",
    "                                         temperature=0.99)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
